
\chapter{Setup}
%Compilation, running, make files, etc.
Short guides for Building and Running EC-Earth 3 on various machines is described at~\cite{br-wiki}.

Prior to compilation step, EC-EARTH build scripts need to be adjusted to the local environment. 
First, the source code of EC-Earth v3.2 HiRes development branch can be obtained from the svn repository: 
    
    \begin{center}
      \code{svn checkout \serifurl{http}{https://svn.ec-earth.org/ecearth3/branches/development/2016/r2811-hires}}
    \end{center}
    
Note that the main trunk is located at:

    \begin{center}
      \code{svn --username XX checkout \serifurl{http}{https://svn.ec-earth.org/ecearth3/trunk}}
    \end{center}


Secondly, make sure that \code{\gls{ExpandNodeList}} and \code{\gls{makedepf90}} are 
in the local \code{bin} folder. 

The file \code{build-config.xml} in \code{sources} folder contains environmental variables and paths for the different platforms. 
KNMI works with the \emph{neuron} platform. The paths should be set properly, in particular, \code{ECEARTH\_SRC\_DIR} and libraries -- 
base directories of:
\begin{center}
\begin{tabular}{ll}
MPI & \code{MPI\_BASE\_DIR}\\ 
LAPACK & \code{LAPACK\_BASE\_DIR}\\ 
JPEG used for compression in HDF & \code{JPEG\_BASE\_DIR}\\ 
SZIP & \code{SZIP\_BASE\_DIR}\\ 
Hdf4 & \code{HDF4\_BASE\_DIR}\\ 
Hdf5 & \code{HDF5\_BASE\_DIR}\\ 
NetCDF & \code{NETCDF\_BASE\_DIR}\\ 
GRIB API & \code{GRIBAPI\_BASE\_DIR}\\ 
GRIBEX & \code{GRIBEX\_BASE\_DIR}\\ 
F90 dependency generator & \code{MAKEDEPF90}
\end{tabular}
\end{center}


\begin{dependency}
\begin{deptext}
JPEG-6B \& SZIP \& HDF4 \& HDF5 \& NetCDF \& sGRIB \\
\end{deptext}
\depedge{2}{3}{ZLIB}
\end{dependency}

For the development guide, see~\cite{dev-guide}.


\section{Input data}
%   Resolution description, its list and the description of each file, grid type
\gls{IFS} produces \gls{GRIB} files as an output and \gls{NEMO} outputs \gls{netCDF} files via XiOS. 
The output variables are defined through \code{pptdddddd0000} for IFS and \code{namelistXX.cfg.sh} (which is converted to XML file) for \gls{NEMO}.
The output data is then \gls{CMOR}ized.

Since \gls{NEMO} and \gls{IFS} are coupled through OASIS \cite{TM673}
The \texttt{namcouple.sh} file contains the following information regarding \gls{IFS} and \gls{NEMO}:

\begin{lstlisting}
    159) atm_grid=A080
        atm_nx=35718
        ;;
    255) atm_grid=A128
        atm_nx=88838
        nem_nx=362
        nem_ny=292
        ;;
    511) atm_grid=A256
        atm_nx=348528
        nem_nx=1442
        nem_ny=1021
        ;;
    799) atm_grid=A400
        atm_nx=843490  
\end{lstlisting}




% Ocean receives:
% wind stress over ocean and ice, solar and non-solar fluxes, 
% liquid and solid precipitation, total evaporation and evaporation over ice,
% sensitivity of non-solar heat flux over ice, 
% runoff into ocean, calving of ice  
% Atmosphere receives: 
% Sea surface temperature, ice temperature, ice albedo, ice fraction and thickness
% Snow thickness on ice


%CALV_OCE" "QS___OCE" "QS___ICE" "QNS__OCE" "QNS__ICE

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
 & IFS/NEMO (w/o LIM3) & IFS/NEMO (w LIM3) \\
 \hline
IFS to NEMO & Solar radiation & Ocean solar radiation \\
& & Ice solar radiation \\
& Non-solar radiation & Ocean non-solar radiation\\
& & Ice non-solar radiation\\
& Evaporation minus precipitation & Total evaporation \code{EVAP\_TOT}\\
& & Evaporation over ice \code{EVAP\_ICE}\\
& & Liquid precipitation \code{PRCP\_LIQ}\\
& & Solid Precipitation \code{PRCP\_SOL}\\
& & Runoff \code{RNF\_\_OCE}\\
& Zonal wind stress & Ocean zonal stress \code{TAUX\_OCE} \\
& & Ice zonal stress \code{TAUX\_ICE}\\
& Meridional wind stress & Ocean meridonal stress \code{TAUY\_OCE}\\
& & Ice meridonal stress \code{TAUY\_ICE} \\
& & Ice dQ/dT \code{DQDT\_ICE}\\
\hline
NEMO to IFS & Sea surface temperature & Sea surface temperature\\
& Sea ice concentration & Sea ice concentration\\
& & Sea ice temperature\\
& & Sea ice albedo\\
& & Sea ice thickness\\
& & Snow thickness over sea ice\\
& Zonal surface current & Zonal surface current \\
& Meridional surface current & Meridional surface current  \\
\end{tabular}
\caption{FIXME: Fields to be exchanged via OASIS coupling between IFS and NEMO with(out) LIM3} 
\end{table}

Input and output in NEMO is dictated by the \code{namelist} parameters.

\gls{nudging} and \gls{forcing}

\section{Compilation}

The compilation step requires:
\begin{itemize}
    \item Fortran 90 - fortran compiler
    ...
\end{itemize}



After setting up proper paths in \code{config-build.xml}, the compilation proceeds as follows:
\begin{verbatim}
    cconf32 & coas32 & cxios32 & crunoff32 & cifs32 & cnemo32  
\end{verbatim}
For more details on these functions, see Listing~\ref{app:comp} in the appendix. 
Note that XiOS is only compiled with g++ and its fortran interface is compiled with the available fortran compiler. As discussed below, it is not possible to use gfortran for the latter. Do not forget to run cconf32 if something is changed in config-build.xml file.

We have tested a local compilation of ec-earth 3.2b with the GNU compiler suite (gcc, g++ and gfortran) and the openMPI implementation of the message passing interface. In the build configuration file it is necessary to assign the following parameter values:
\begin{center}
\begin{tabular}{ll}
\texttt{FC}&\texttt{gfortran}\\
\texttt{FFLAGS}&\texttt{-O2 -g -fdefault-real-8 -fdefault-double-8 -ffree-line-length-none}\\
\texttt{FPP}&\texttt{gfortran -cpp}\\
\texttt{FFLAGS\_FREEFORM}&\texttt{-ffree-form}\\
\texttt{FFLAGS\_FIXEDFORM}&\texttt{-ffixed-form}\\
\texttt{FFLAGS\_FPP\_PREFIX}&\texttt{-D}\\
\texttt{MPI\_LIBS\_WITHOUT\_L}&\texttt{mpi mpi\_cxx mpi\_f77 mpi\_f90}\\
\end{tabular}
\end{center}
Unfortunately the Xios-1.0 component will not compile with the above settings in the current revision (2882). This is due to the fact that it uses token concatenation in its Fortran interface files, which is not substituted by gcc. Hence we advice strongly to install ifort before compiling ec-earth including Xios at this point. 

\section{Running}
There are three options of running EC-Earth on Bull with SLURM system~\cite{slurm}:
\begin{itemize}
  \item \code{sub32atm} for the atmosphere standalone model;    
  \item \code{sub32oce} for the ocean standalone model;    
  \item \code{sub32} for the coupled model.
\end{itemize}
These functions are presented in Listing~\ref{app:run} in the appendix. All functions have an option \code{-hr} for runs in high resolution. 